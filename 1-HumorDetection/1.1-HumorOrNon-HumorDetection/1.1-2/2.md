# Humor Recognition and Humor Anchor Extraction

首先确定幽默语义结构（不一致性、歧义、人际影响、音韵风格），为每个语义结构设计幽默文体特征来识别幽默；利用最大减量算法来提取幽默点。

#### Author

- **Diyi Yang, Alon Lavie, Chris Dyer, Eduard Hovy**: Language Technologies Institute, School of Computer Science Carnegie Mellon University

#### Abstract

Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.

> ### Overview
>
> **Model Name**:
>
> - Word2Vec+HCF(Human Centric Features)
>
> **Motivation**:
>
> - 构建四个幽默特征，来进行幽默识别并且提出最大减量算法来提取笑点
>
> **Structure**:
>
> - 构建四个幽默特征进行幽默识别，特别地是提取了笑点
>
> **Method**:
>
> - Latent Structures behind Humor
>   - Incongruity Structure
>   - Ambiguity Theory
>   - Interpersonal Effect
>   - Phonetic Style
> - Humor Anchor Extraction
>   - 比较去掉词语对句子幽默影响
>
> **Experiment**:
>
> - 在本文提出的数据集上取得了不错的效果，并且提出了一种评价指标来衡量笑点提取的效果
>
> **Brief Comment**:
>
> - 构建了幽默特征
> - 提出了笑点提取任务

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
In this work, we formulate humor recognition as a classification task in which we distinguish between humorous and non-humorous instances. Then we explore the semantic structure behind humor from four perspectives: incongruity, ambiguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance.

### Related Work
In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences. A certain type of jokes might have specific structures or characteristics that provide pointers to humor anchors. For example, in the problem of “That’s what she said” (Kiddon and Brun, 2011), characteristics that involves the using of nouns that are euphemisms for sexually explicit nouns or structures common in the erotic domain might probably give clues to potential humor anchors. Similarly, in the Knock Knock jokes (Taylor and Mazlack, 2004), wordplay is what leads to the humor. However, the wordplay by itself is not enough to trigger the comic effect, thus not equivalent to the humor anchors for a joke. To address these issues, we introduce a formal definition of humor anchors and design an effective method to extract such anchors in this work. To the best of our knowledge, this is the first study on extracting humor anchors that trigger humor in general sentences.

### Datasets
<div align="center">
  <image src="img/1.png" width="700">
</div>

### Latent Structures behind Humor
- Incongruity Structure
Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013):
    - Disconnection: the maximum meaning distance of word pairs in a sentence.
    - Repetition: the minimum meaning distance of word pairs in a sentence.
- Ambiguity Theory
The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows:
    - Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {${w_1,w_2 · · ·w_k}$} via WordNet and calculate the sense combinations as $log(\prod_{i=1}^k n_{w_i})$. $n_{w_i}$ is the total number of senses of word $w_i$.
    - Sense Farmost: the largest Path Similarity of any word senses in a sentence.
    - Sense Closest: the smallest Path Similarity of any word senses in a sentence.
- Interpersonal Effect
Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features.
    - Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words.
    - Weak (Strong) Subjectivity: the number of occurrences of allWeak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations.
- Phonetic Style
An alliteration chain refers to two or more words beginning with the same phones. A rhyme chain is defined as the relationship that words end with the same syllable. To extract this phonetic feature, we take advantage of the CMU Pronouncing Dictionary 8 and design four features as follows:
    - Alliteration: the number of alliteration chains in a sentence, and the maximum length of alliteration chains.
    - Rhyme: the number of rhyme chains and the maximum length of rhyme chains.

### Humor Anchor Extraction
- Humor Anchor Definition
The semantic units or humor anchors enable humor in a given sentence, and are reflected in the form of sentence words. However, not every single word can be a humor anchor. For example, I am glad that I know sign language; it is pretty handy. In this one-liner, words such as ‘am’ and ‘is’ are not able to enable humor via themselves. Similarly, ‘sign’ or ‘language’ itself are not capable to prompt comic effect. The possible anchors in this example should contain both ‘sign language’ and ‘handy’; it is the combination of these two spans that triggers humor. Therefore, formally defined, a humor anchor is a meaningful, complete, minimal set of word spans in a sentence that potentially enable one of the latent structures of Section 4 to occur. 
    - (1) Meaningful means humor anchors are meaningful word spans, not meaningless stop words in a sentence; 
    - (2) Completeness shows that all possible humor anchors should be covered by this anchor set and no individual span in this anchor set is capable enough to enable humor; 
    - (3) Minimal emphasizes that it is the combination of these anchors together that prompts comic effect; discarding any anchors from this candidate set destroys the humorous effect.
- Anchor Extraction Method
Mathematically, $X_i$ is the word set of sentence $i$. Let $f$ denote the trained classifier on all data instances. $f(X_i)$ is the predicted humor score for sentence $i$ before performing any operations. Denote ${K_i}$$({K_i}\subset{X_i})$ as the subset of words that we need to remove from sentence $i$. The size of $K_i$ should be smaller than a threshold $t$, $|Ki|\leq{t}$. $f(X_i/K_i)$ is the recomputed humor score for sentence $i$ after removing $K_i$. Our Maximal Decrement method tries to maximize the following objective by enumerating all possible ${K_i}$s. The subset $K_i$ that gives the maximal decrement is returned as our extracted humor anchors for sentence $i$. The system overview is shown in Figure.
$$ arg {\mathop{min}\limits_{|K_i|\leq{t}}} f(X_i)-f(X_i/K_i) $$
<div align="center">
  <image src="img/2.png" width="700">
</div>
### Experiment
- Humor Recognition
<div align="center">
  <image src="img/3.png" width="700">
</div>
In addition to the four latent structures behind humor, we also design a set of K Nearest Neighbor (KNN) features that uses the humor classes of the K sentences (K = 5) that are the closest to this sentence in terms of meaning distance in the training data. We use several methods to act as baselines for comparison with our classier. Bag of Words baseline is used to capture a multiset of words in a sentence that might differentiate humor and non-humor. Language Model baseline assigns a humor/nonhumor probability to words in a sentence via probability distributions. Word2Vec baseline represents the meaning of sentences via Word2Vec (Mikolov et al., 2013) distributional semantic meaning representation. We implemented an earlier work (Mihalcea and Strapparava, 2005) that exploits stylistic features including alliteration, autonomy and adult slang and ensembles with bag of words representations, denoted as SaC Ensemble. It is worth mentioning that our datasets are balanced in terms of positive and negative instances, giving a random classification accuracy of 50%.
<div align="center">
  <image src="img/4.png" width="700">
</div>

- Anchor Extraction
    - Qualitative Evaluation
    To better understand which words or semantic units enable humor in sentences, we performed humor anchor extraction as described in Section 5.2. We set the size of the humor anchor set as 3, i.e. $t = 3$. The classifier that is used to predict the humor score is trained on all data instances. Then all predicted humorous instances are collected and input into the humor anchor extraction component. Based on the Maximal Decrement method, a set of humor anchors is extracted for each instance.
    <div align="center">
        <image src="img/5.png" width="700">
    </div>

    - Quantitative Evaluation
    For each dataset, we randomly sampled 200 sentences. Then for each sentence, 3 annotators are asked to annotate and label the possible humor anchors. To assess the consistency of the labeling in this context, we introduced an Annotation Agreement Ratio (AAR) measurement as follows:
    $$ AAR(A, B)=\frac{1}{N_s}\sum^{N_s}_{i=1}\frac{|A_i\cap{B_i}|}{|A_i\cup{B_i}|} $$
    As a further step to validate the effectiveness of our anchor extraction method, we also introduced two baselines. The Random Extraction baseline selects humor anchors by sampling words in a sentence randomly. Similarly, POS Extraction baseline generates anchors by narrowing down all the words in a sentence to a set of certain POS, e.g. Noun, Verb, Noun Phrase, Verb Phrase, ADVP and ADJP and then sampling words from this set. To evaluate whether our extracted anchors are consistent with human annotation, we used each annotator’s extracted anchor list as the ground truth, and compared with anchor list provided by our method. To identify whether two anchors are the same, we introduce two measurements: Exact (EX) Matching and At-Least-One (ALO) Matching. Exact Matching requires the two anchors to be exactly the same. For ALO, two anchors are considered the same if they have at least one word in common. Recall, Precision and F1 Score are act as evaluation metrics. We then average the three annotators’ individual scores to get the final extraction performance.
    <div align="center">
        <image src="img/6.png" width="700">
    </div>
      
      
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
formula1: $$n==x$$

formula2: $$n!=x$$

formula3: (m==y)

formula4: [m!=y]

formula5: \(k==z\)

formula6: \[k!=z\]
