# A Long Short-Term Memory Framework for Predicting Humor in Dialogues

预测对话中的幽默（生活大爆炸），每个句子用CNN编码后，LSTM针对对话的上下文序列来建模。

#### Author

- **Dario Bertero and Pascale Fung**: Human Language Technology Center Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology

#### Abstract

We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.

> ### Overview
>
> **Model Name**:
>
> - CNN+句子级LSTM
>
> **Motivation**:
>
> - 自动化幽默对话是被是构建有效的情感机器的第一步，该机器完全能够对用户的幽默和其他话语刺激做出反应。
>
> **Structure**:
>
> - CNN提取词语级特征，输入到LSTM中学习对话间信息
>
> **Method**:
>
> - CNN中3种词语特征：
>   - Word tokens
>   - Character trigrams
>   - Word2Vec
> - LSTM输出结合一些幽默特征：
>   - Structural features 
>   - Part of speech proportion
>   - Antonyms
>   - Sentiment
>   - Speaker and turn
>   - Speaking rate
>
> **Experiment**:
>
> - 生活大爆炸数据集，实验结果在F1和准确率上取得了不错的效果，但是其中对分类影响比较大的是幽默特征的构建
>
> **Brief Comment**:
>
> - 提供对话级LSTM的思路，最后分类结果中更加关键的是幽默特征的选取

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
We analyze transcripts of funny dialogues, a genre somehow neglected but important for human-robot interaction. Laughter is the natural reaction of people to a verbal or textual humorous stimulus. We want to predict when the audience would laugh. Compared to a typical canned joke or a sarcastic Tweet, a dialog utterance is perceived as funny only in relation to the dialog context and the past history. In a spontaneous setting a funny dialog is usually built through a setup which prepares the audience to receive the humorous discourse stimuli, followed by a punchline which releases the tension and triggers the laughter reaction (Attardo, 1997; Taylor and Mazlack, 2005). Automatic understanding of a humorous dialog is a first step to build an effective empathetic machine fully able to react to the user’s humor and to other discourse stimuli. We are ultimately interested in developing robots that can bond with humans better (Devillers et al., 2015).

Our previous attempts on the same corpus (Bertero and Fung, 2016b; Bertero and Fung, 2016a) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network (Hochreiter and Schmidhuber, 1997), where we encode each sentence through a Convolutional Neural Network (Collobert et al., 2011). LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (Graves et al., 2013), dependency parsing (Dyer et al., 2015) and conversation modelling (Shang et al., 2015). This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks.

### Methodology
- Convolutional Neural Network for each utterance

    - Word tokens: each utterance token is represented as a one-hot vector. This feature models how much each word is likely to trigger humor in the specific corpus.
    - Character trigrams: each token is represented as a bag-of-character-trigrams vector. The feature models the role of the word signifier and removes the influence of the word stems.
    - Word2Vec: we extract for each token a word vector from word2vec (Mikolov et al., 2013), trained on the text9 Wikipedia corpus1. This representation models the general semantic

- Long/Short Term Memory for the utterance sequence

    Before the output we incorporate a set of high level features from our previous work (Bertero and Fung, 2016b) and past literature (Reyes et al., 2013; Barbieri and Saggion, 2014). They include:

    - Structural features: average word length, sentence length, difference in sentence length with the five previous utterances.
    - Part of speech proportion: noun, verbs, adjectives and adverbs.
    - Antonyms: proportion of antonym words with the previous utterance (from WordNet (Miller, 1995)).
    - Sentiment: positive, negative and average sentiment score among all words (from Senti- WordNet (Esuli and Sebastiani, 2006)).
    - Speaker and turn: speaker character identity and utterance position in the turn (beginning, middle, end, isolated).
    - Speaking rate: time duration of the utterance from the subtitle files, divided by the sentence length.

    All these features are concatenated to the LSTM output, and a softmax layer is applied to get the final output probabilities.

- Model
    <div align="center">
        <image src="img/1.png" width="700">
    </div>

### Experiment
- Corpus
We built a corpus from the popular TV-sitcom “The Big Bang Theory”, seasons 1 to 6.We downloaded the subtitle files (annotated with the timestamps of each utterance) and the scripts, used to segment all the episodes into scenes and get the speaker identity of each utterance.(From bigbangtrans.wordpress.com)
- Results
Results of our system and our baselines are shown in table 1. The LSTM with the aid of the high level feature vector generally outperformed all the CRF baselines with the highest accuracy of 70.0% and the highest F-score of 62.9%. The biggest improvement of the LSTM is the improvement of the recall without affecting too much the precision. Lexical features given by n-gram from a context window are very useful to recognize more punchlines in our baseline experiment, but they also yield many false positives, when the same n-gram is used in different contexts. A CNN-LSTM network seems to overcome this issue as the CNN stage is better in modeling the lexical and semantic content of the utterance, as the LSTM allows to put each utterance in relation with the past context, filtering out many false positives from wrong contexts.
<div align="center">
    <image src="img/2.png" width="700">
</div>
The choice of the CNN is further justified by the results obtained from the comparison between the CNN and the LSTM sentence encoding input, shown in table 2. The CNN is more effective, obtaining a recall of 10% higher and 6% more in Fscore. The CNN is a simpler model that might benefit more of a small-size corpus. It also required a much shorter training time compared to the LSTM. We may consider in the future to use more data, and try other sentence input encoders, including deeper or bi-directional LSTMs, to find the most effective one.
<div align="center">
    <image src="img/3.png" width="700">
</div>
