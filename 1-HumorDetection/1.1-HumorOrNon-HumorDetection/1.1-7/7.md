# Recognizing Humour usingWord Associations and Humour Anchor Extraction

使用词关联性的语义特征，而不是Word2Vec相似度，来进行幽默识别并提取笑点

#### Author

- **Andrew Cattle, Xiaojuan Ma**: Hong Kong University of Science and Technology Department of Computer Science and Engineering

#### Abstract

This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.

> ### Overview
>
> **Model Name**:
>
> - Random Forest Classifier
>
> **Motivation**:
>
> - 从笑话结构和笑话语义来构建可解释的机器学习方法
>
> **Structure**:
>
> - RF(随机森林)
>
> **Method**:
>
> - 计算词语之间的关联度，如"beer"和"glass"之间，语义不相关，但是"beer"经常使用"glass"装，这之间却是相关的
>
> **Experiment**:
>
> - 在Pun of the Day和the 16000 One-Liner数据集上，对比了不同的词关联度方法和相似度方法以及在不同的知识库上的效果
>
> **Brief Comment**:
>
> - 提出了使用词关联度来进行幽默识别

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
This paper aims to marry the interpretability of statistical machine learning approaches with the more nuanced models of joke structure and joke semantics of neural approaches. Specifically, we explore the effectiveness modelling joke semantics using semantic relatedness features based on word associations, rather than the more common semantic similarity features based on word embeddings. We present evidence not only that relatedness in general is better suited than similarity for computational humour tasks due to its broader nature, but also that word associations are particularly well suited due to their ability to map more nuanced relationships (De Deyne and Storms, 2008) and asymmetric nature. While such features have been explored in the past (Cattle and Ma, 2016; Cattle and Ma, 2017b), this work presents a more in depth analysis, focusing on a more fundamental task (binary humour classification vs. relative humour ranking) on with a dataset that better represents natural language (oneliners and puns vs. Twitter hashtag games), and is the first work to incorporate interpolated word association strengths. Furthermore, we introduce a novel method for targetting our semantic features using joke structure to help reduce noise and increase reliability. Specifically, we experiment with integrating the extraction of humour anchors, the “meaningful, complete, minimal set of word spans” (Yang et al., 2015) that allow humour to occur, into the humour classification process itself, the first work to do so. We are also making the code used to run these experiments publicly available.

### Related Work
- **Joke Semantics**
Word associations capture relationships between concepts, are not reliant on distributional semantics, and have long been used in psychology and cognitive science dating back to Galton (1879). Furthermore, some word association-based metrics have been shown to perform better than Word2Vec on a series of similarity tasks (De Deyne et al., 2016). Although the collection of word association datasets is much more labour intensive than distributional semantic approaches, several word association datasets do exist, the most popular being the Edinburgh Associative Thesaurus (**EAT**), collected in Kiss et al. (1973), and the University of South Florida Free Association Norms (**USF**), collected in Nelson et al. (2004). Both these datasets were compiled by presenting participants with a cue word and asking them to respond with the first word that comes to mind. The proportion of respondents who give a specific response for a specific cue is called the forward strength from the cue to the target. This approach does have its drawbacks, forward strengths are relative instead of absolute (Nelson et al., 2004) and accepting only a single response causes weaker relationships to be under-represented (De Deyne and Storms, 2008), but it is a straightforward and widely accepted way of collecting word association data.
- **Joke Structure**
As mentioned in Section 1, joke structure and joke semantics are not entirely independent. This means poor models of joke structure can affect the performance of features designed to capture joke semantics. Despite the issues mentioned in Section 2.1, Yang et al. (2015)’s “incongruity” feature set, maximum and minimum word embedding similarities between pairs of words in a document, perform fairly well. Cattle and Ma (2017b) takes a similar approach with their word association features. The problems comes from the fact that both works compute these values across all possible pairs of words in a document. This can introduce noise as not all word pairs are meaningful (e.g. pairs of stopwords) and internally-cohesive setups and punchlines can bias maximum similarity scores. While this can be somewhat alleviated by judicial filtering of stopwords, this does not guarantee meaningful word pairs either. 
\
Yang et al. (2015), in addition to their humour classifier, also introduces a method for identifying jokes’ humour anchors (HAs), the “meaningful, complete, minimal set of word spans” that allow humour to occur. While this is slightly different from identifying a joke’s setup and punchline, focusing only on pairs of HAs would help reducing noise by increasing the precision of meaningful word pairs selection without sacrificing recall. However, Yang et al. (2015) does not use their extracted HAs to improve their humour classification performance. Likely this is due to the fact that their proposed HA extraction method requires a separate, fully trained humour prediction model which is robust to word order. Furthermore, the quality of the extracted HAs is directly tied to the quality of the humour model. However, both issues could have been alleviated using some form of co-training or bootstrapping between the overall humour prediction model and the HA extractor’s internal humour prediction model. The HA extractor works by generating a list of HA candidates for each document following a heuristic method. After removing various combinations of HA candidates from the original document, these modified documents are fed into a trained humour prediction model, with the HAs being the combination of HA candidates which causes the largest drop in humour score. Since the humour prediction model is by design robust against word order, words can be freely omitted with few side effects.

### Methodology
- **Datasets**
We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in Yang et al. (2015), and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005).
- **Semantic Features**
For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the **forward** strength. Since, as described in Section 2.1, word associations are directional, we also compute the minimum, maximum, and average associations strengths between the reverse ordered word pairs, which we refer to as the **backward** strength. Following Cattle and Ma (2016), we also compute the difference between these two values on both a **micro (per word**) and **macro (per document)** level. We refer to these sets of features as the **diff** strengths. Forward, backward, and diff strengths are extracted for both the EAT and USF word association datasets.
\
We also compare two methods for computing our word association strength features. The first method uses the graph-based method described in Cattle and Ma (2016). We refer to this approach as graph. The second uses an in-house association strength predictor using an machine learning-based method similar to the one described in Cattle and Ma (2017a). We refer to this approach as ML. Unlike Cattle and Ma (2017a), which uses three different types of word embeddings, we use only Word2Vec. In addition to Word2Vec similarity and vector offsets (the difference between the cue and response vectors), we also include LDA similarity (300 dimensions, trained using Gensim6 on English Wikipedia), AutoExtend (Rothe and Sch¨utze, 2015) similarity, and a variety of WordNet-based features described in Cattle and Ma (2017a).
- **Humour Classifier**
In addition to the semantic features described above, we also calculate each document’s perplexity according to a 3gram language model (LM) trained on the WMT15 English news discussion corpus (Bojar et al., 2015) using KenLM7. This feature is included not only because document perplexity has been shown to be useful for humour recognition (Shahaf et al., 2015) but also to act as a baseline for our semantic features. Because both the datasets described in Section 3.1 draw negative examples from news sources, we were concerned that training an LM on a general English corpus might unfairly bias the perplexity scores against those negative examples. Similarly, training an LM on a news corpus might unfairly bias perplexity towards those negative examples. Therefore, the news discussion corpus was chosen as a happy medium between news vocabulary and informal writing styles.
\
As with our baseline, our extracted features are used to train a Random Forest Classifier using scikitlearn with 100 estimators and default settings using a 10 fold cross validation.
- **Humour Anchors**
By default, word association and Word2Vec features are computed across all word pairs in a document. However, we also experiment computing these features only across pairs of humour anchor (HA) words. HAs are extracted using the method described in Yang et al. (2015) using the same baseline humour model described in Section 3.2 for anchor candidate evaluation.
\
HA extraction’s requirement of a fully trained anchor candidate evaluator raises the problem of what data that evaluator should be trained on. Given that, as described in Section 3.1, we experiment on two separate humour datasets, we train the anchor candidate evaluator on the opposite dataset from the overall humour classifier. This is to avoid overfitting or biasing the anchor candidate evaluator by training on the test data.

### Experiment
- **Humour Classification**
<div align="center">
    <image src="./img/1.png" width="700">
</div>

- **Humour Anchors**
<div align="center">
    <image src="./img/2.png" width="700">
</div>
