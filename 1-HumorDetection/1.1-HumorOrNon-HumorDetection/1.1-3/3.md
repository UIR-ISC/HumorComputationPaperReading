# Deep Learning of Audio and Language Features for Humor Prediction

预测对话中的幽默（生活大爆炸），采用对比研究CRF（时间序列）、RNN和CNN并分别结合语音特征和语言特征的效果。

#### Author

- **Dario Bertero, Pascale Fung**: Human Language Technology Center Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology

#### Abstract

We propose a comparison between various supervised machine learning methods to predict and detect humor in dialogues. We retrieve our humorous dialogues from a very popular TV sitcom: “The Big Bang Theory”. We build a corpus where punchlines are annotated using the canned laughter embedded in the audio track. Our comparative study involves a linear-chain Conditional Random Field over a Recurrent Neural Network and a Convolutional Neural Network. Using a combination of word-level and audio frame-level features, the CNN outperforms the other methods, obtaining the best F-score of 68:5% over 66:5% by CRF and 52:9% by RNN. Our work is a starting point to developing more effective machine learning and neural network models on the humor prediction task, as well as developing machines capable in understanding humor in general.

> ### Overview
>
> **Model Name**:
>
> - CRF、CNN、RNN
>
> **Motivation**:
>
> - 结合语音和语言特征来解决幽默识别问题
>
> **Structure**:
>
> - 分别在CNN和RNN结构中加入了语音和语言特征
>
> **Method**:
>
> - Acoustic features
> - Language features
>
> **Experiment**:
>
> - 构建了生活大爆炸数据集
> - 对比不同模型不同特征的影响
>
> **Brief Comment**:
>
> - 对比了不同特征不同模型对分类结果的影响

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
In order to fully take advantage of the dialog context, we employ and compare three different classification algorithms: a Conditional Random Field, a Recurrent Neural Network and a Convolutional Neural Network. We train the former two with a set of acoustic and language features, while in the latter we replace some of the features with low level representations of words and acoustic frames. 
Predicting when people would react to humor and laugh is an important problem with potential great implications in human-machine interaction. A system that predict humor is a foundational block for future empathetic machines able to effectively understand and react to humorous stimuli provided by the user (Fung, 2015).

### Methodology
- **Acoustic features**
In a multimodal dialog variations in pitch, loudness and intonation often indicate whether the intent of the speaker is serious or humorous. To model this aspect we retrieve a set of around 2500 acoustic features from the openSMILE software (Eyben et al., 2013) using the emobase and emobase2010 packages provided (made of the feature set from the INTERSPEECH 2010 paralinguistic challenge (Schuller et al., 2010)). These features include MFCC, pitch, intensity, loudness, probability of voicing, F0 envelope, Line Spectral Frequencies, Zero-Crossing rate and their variations (delta coefficients).
Another element that is associated with humor is the speed at which an utterance is said. Talking deliberately too slowly may make fun of the recipient, while a deliberate fast pace may prevent the listener to catch all the information and trigger violation of Gricean Maxim of manner (Attardo, 1993). We therefore include the speaking rate of the utterance (time duration divided by the number of words) to our feature set.

- **Language features**
We also retrieve a set of language features from the utterance transcriptions. They represent multiple aspects, ranging from syntax to semantic and sentiment. The features we use are:
    - Lexical: unigrams, bigrams ans trigrams that appear 5 times or more.
    - Syntactic and structural (Barbieri and Saggion, 2014): proportion of nouns, verbs, adjectives and adverbs, sentence length, length difference with the previous utterance and average word length.
    - Sentiment (Barbieri and Saggion, 2014): average of positive sentiment scores and negative sentiment scores from SentiWordNet, average of all scores and difference between the positive and negative averages.
    - Antonyms: presence of noun, verb, adjective and adverb antonyms in the previous utterance, obtained from WordNet (Miller, 1995).
    - Speaker turns: speaker identity and position within the speaker turn (beginning, middle, end, isolated). Various speakers are more or less likely to generate humor (as shown in figure 4).

- **CRF**
We use a standard linear chain CRF to model our dialog, which can be summarized with the following equation:
$$ p(y|x)=\frac{1}{Z(x)}\prod\limits_{A}exp\{\sum\limits_k{\theta_{Ak}f_{Ak}(x_A,y_A)} \}$$
where $A$ represents the graph nodes, $k$ is the feature index, $x$ is the total observation, $\theta_{Ak}$ are the model parameters to be trained, $f_{Ak}$ are the feature functions and $Z(x)$ a normalization
function.

- **RNN**
<div align="center">
  <image src="img/1.png" width="700">
</div>

- **CNN**
<div align="center">
  <image src="img/2.png" width="700">
</div>

### Experiment
- **Corpus**
Overall the corpus contains 1589 scenes. The episodes were divided into a training set of around 35865 overall utterances, and a development set of 3904 and test set of 3903. The corpus consist of 42:8% of the utterances being punchlines. The average interval between two of them is 2.2 utterances, figure 5 shows the overall interval distribution. There are 7 recurring characters appearing for more than 500 utterances. As shown in figure 4 the amount of punchlines associated to each character is different by over 20%. We grouped all characters other than the seven most frequent into the “other” label for the speaker identity feature.
    <div align="center">
        <image src="img/3.png" width="700">
    </div>
    <div align="center">
        <image src="img/4.png" width="700">
    </div>

- **Results**
<div align="center">
  <image src="img/5.png" width="700">
</div>
