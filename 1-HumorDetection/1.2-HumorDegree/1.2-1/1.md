# DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison

在给定的两条Twitter消息中确定哪一条更有趣。使用LSTM孪生结构为每条推特生成一个密集的向量表示，将两条推特向量拼接在一起进行分类。

#### Author

- **Christos Baziotis, Nikos Pelekis, Christos Doulkeridis**: University of Piraeus - Data Science Lab

#### Abstract

In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 “#HashtagWars: Learning a Sense of Humor”. We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-theart results on #HashtagWars dataset.

> ### Overview
>
> **Model Name**:
>
> - BiLSTM+Attention
>
> **Motivation**:
>
> - 在SemEval-2017 Task 6 “#HashtagWars: Learning a Sense of Humor”中的子任务A(比较两条文本幽默度)，取得了最佳效果
>
> **Structure**:
>
> - Siamese Bidirectional LSTM with context-aware attention mechanism
>
> **Method**:
>
> - 将两条tweet的注意力机制输出拼接并分类
>
> **Experiment**:
>
> - 在子任务A上提交结果为第二，在后续bug-fix版本中取得了最佳结果
>
> **Brief Comment**:
>
> - 通过对不同的幽默文本提取特征，并将特征向量进行拼接来实现分类和比较

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
In this paper, we present a deep-learning system that we developed for subtask A - “Pairwise Comparison”. The goal of the task is, given two tweets about the same topic, to identify which one is funnier. The labels are applied using the show’s relative ranking. This is a very challenging task, because humor is subjective and the machine learning system must develop a sense of humor similar to that of the show, in order to perform well.
\
We employ a Siamese neural network, which generates a dense vector representation for each tweet and then uses those representations as features for classification. For modeling the Twitter messages we use Long Short-Term Memory (LSTM) networks augmented with a contextaware attention mechanism (Yang et al., 2016). Furthermore, we perform thorough text preprocessing that enables our neural network to learn better features. Finally, our approach does not rely on any hand-crafted features.

### System Overview
- **External Data andWord Embeddings**
We collected a big dataset of 330M English Twitter messages, which is used (1) for calculating word statistics needed for word segmentation and spell correction and (2) for training word embeddings. Word embeddings are dense vector representations of words (Collobert and Weston, 2008; Mikolov et al., 2013), capturing their semantic and syntactic information. We leverage our big Twitter dataset to train our own word embeddings, using GloVe (Pennington et al., 2014). The word embeddings are used for initializing the weights of the first layer (embedding layer) of our network.
- **Text Preprocessing**
    - **Tokenizer**
    Our tokenizer is able to identify most emoticons, emojis, expressions like dates (e.g. 07/11/2011, April 23rd), times (e.g. 4:30pm, 11:00 am), currencies (e.g. $10, 25mil, 50e), acronyms, censored words (e.g. s**t), words with emphasis (e.g. *very*) and more. This way we keep all these expressions as one token, so later we can normalize them, or annotate them (with special tags) reducing the vocabulary size and enabling our model to learn more abstract features.
    - **Postprocessing** 
    After the tokenization we add an extra postprocessing step, where we perform spell correction, word normalization, word segmentation (for splitting a hashtag to its constituent words) and word annotation. We use the Viterbi algorithm in order to perform spell correction (Jurafsky and Martin, 2000) and word segmentation (Segaran and Hammerbacher, 2009), utilizing word statistics (unigrams and bigrams) from our big Twitter dataset. Finally, we lowercase all words, and replace URLs, emails and user handles (@user), with special tags.
- **LSTM**
- **Attention Mechanism**

### Model Description
The network has two inputs, the sequence of words in the first tweet $X_1 = (x^1_1, x^1_2, ..., x^1_{T_1})$, where $T_1$ the number of words in the first tweet, and the sequence words of the second tweet $X_2 =(x^2_1, x^2_2, ..., x^2_{T_2})$, where $T_2$ the number of words of the second tweet.
- **Embedding Layer**
We use an Embedding layer to project the words to a low-dimensional vector space $R^E$, where $E$ is the size of the Embedding layer. We initialize the weights of the Embedding layer using our pre-trained word embeddings.
- **BiLSTM Layer**
An LSTM takes as input the words of a tweet and produces the word annotations $H = (h_1, h_2, ..., h_T )$, where $h_i$ is the hidden state of the LSTM at time-step $i$, summarizing all the information of the sentence up to $x_i$. We use bidirectional LSTM (BiLSTM) in order to get annotations for each word that summarize the information from both directions of the message. A bidirectional LSTM consists of a forward LSTM $\mathop f\limits^\rightarrow$ that reads the sentence from $x_1$ to $x_T$ and a backward LSTM $\mathop f\limits^\leftarrow$ that reads the sentence from $x_T$ to $x_1$. We obtain the final annotation for each word $x_i$, by concatenating the annotations from both directions,
$$h_i = \mathop {h_i}\limits^\rightarrow || \mathop {h_i}\limits^\leftarrow, h_i\in R^{2L}$$
where $||$ denotes the concatenation operation and $L$ the size of each LSTM.
- **Context-Attention Layer**
An attention mechanism assigns a weight $a_i$ to each word annotation, which reflects its importance. We compute the fixed representation $r$ of the whole message as the weighted sum of all the word annotations using the attention weights. We use a context-aware attention mechanism as in (Yang et al., 2016). This attention mechanism introduces a context vector $u_h$, which can be interpreted as a fixed query, that helps to identify the informative words and it is randomly initialized and jointly learned with the rest of the attention layer weights. Formally,
$$e_i = tanh(W_hh_i + b_h), e_i \in [−1, 1]$$
$$a_i = \frac{exp(e_i^\intercal u_h)}{\sum^T_{t=1}exp(e_t^\intercal u_h)}, \sum^T_{i=1}a_i=1$$
$$r =\sum^T_{i=1}a_ih_i, r \in R^{2L}$$
where $W_h$, $b_h$ and $u_h$ are the layer’s weights. 
- **Fully-Connected Layer**
Each Siamese subnetwork produces a fixed representation for each tweet, $r_1$ and $r_2$ respectively, that we concatenate to produce the final representation $r$.
$$r = r_1 || r_2, r \in R^{4L}$$
We pass the vector $r$, to a fully-connected feedforward layer with a $tanh$ (hyperbolic tangent) activation function. This layer learns a non-linear function of the input vector, enabling it to perform the complex task of humor comparison.
$$c = tanh(W_{c}r + b_c)$$
- **Output Layer**
The output $c$ of the comparison layer is fed to a final single neuron layer, that performs binary classification (logistic regression) and identifies which tweet is funnier.

### Experiment
- **Subtask A Results**
<div align="center">
    <image src="img/1.png" width="700">
</div>

- **#HastagWars Dataset Results**
<div align="center">
    <image src="img/2.png" width="700">
</div>
