# SemEval-2020 Task 7: Assessing Humor in Edited News Headlines

对标题幽默程度打分，并比较哪个更幽默。对于子任务1，系统使用五个注释者的有趣程度评分的平均值和系统对标题的估计值之间的均方根误差（RMSE）对系统进行排名；对于子任务2，它试图找到幽默标题的有趣之处来进行比较，评估指标是分类准确性。最终总结了所有蔡塞队伍的结果

#### Author

- **Nabil Hossainy, Henry Kautzy：** Department of Computer Science, University of Rochester
- **John Krummz, Michael Gamonz：** Microsoft Research AI, Microsoft Corporation

#### Abstract

This paper describes the SemEval-2020 shared task “Assessing Humor in Edited News Headlines.” The task’s dataset contains news headlines in which short edits were applied to make them funny, and the funniness of these edited headlines was rated using crowdsourcing. This task includes two subtasks, the first of which is to estimate the funniness of headlines on a humor scale in the interval 0-3. The second subtask is to predict, for a pair of edited versions of the same original headline, which is the funnier version. To date, this task is the most popular shared computational humor task, attracting 48 teams for the first subtask and 31 teams for the second.

> ### Overview
>
> **Model Name**:
>
> - RoBERTa, BERT, CBOW, BASELINE
>
> **Motivation**:
>
> - 对比各系统的结果
>
> **Structure**:
>
> - RoBERTa, BERT, CBOW, BASELINE
>
> **Method**:
>
> - RoBERTa, BERT, CBOW, BASELINE
>
> **Experiment**:
>
> - 设置了RoBERTa, BERT, CBOW, BASELINE的Benchmark与参赛队伍进行对比
>
> **Brief Comment**:
>
> - 总结了各个参赛团队的系统以及结果

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
Ours is the largest shared humor task to date in terms of participation. More than 300 participants signed up, 86 teams participated in the development phase, and 48 and 31 teams participated, respectively, in the two subtasks in the evaluation phase. By creating an intense focus on the same humor task from so many points of view, we were able to clearly understand how well these systems work as a function of different dimensions of humor, including which type of humor appears easiest to rate automatically.

### Datasets
- **Humicroedit**
This dataset contains about 5,000 original headlines, each having three modified, potentially funny versions for a total of 15,095 edited headlines. The original headlines were collected from Reddit (reddit.com) via the popular subreddits r/worldnews and r/politics, where headlines from professional news sources are posted everyday. These headlines were published between 01/2017 and 05/2018, they are between 4-20 words long, and they are sampled from headlines written by 25 major English news sources.

- **FunLines**
We also provided additional training data3 from FunLines4 (Hossain et al., 2020), a competition that we hosted to collect humorous headlines at a very low cost. The data collection approach for Humicroedit and FunLines are mostly similar, but FunLines additionally includes headlines from the news categories sports, entertainment and technology, and its headlines were published between 05/2019 and 01/2020, for a total of 8,248 annotated headlines. More than 40% of the participating teams, including the winning team, made use of the FunLines data.
<div align="center">
    <image src="img/1.png" width="700">
</div>

### Task Description
- **Subtask 1: Funniness Regression**
In this task, given the original and the edited versions of a headline, the participant has to estimate the mean funniness of the edited headline on the 0-3 humor scale. Systems tackling this task can be useful in a humor generation scenario where generated candidates are ranked according to expected funniness.

- **Subtask 2: Funnier of the Two**
In this task, given the original headline and two of its edited versions, the participating system has to predict which edited version is the funnier of the two. Consequently, by looking at gaps between the funniness ratings, we can begin to understand the minimal discernible difference between funny headlines.

### Evaluation
- **Metrics**
    - **Subtask 1**
    $$ RMSE=\sqrt{\frac{\sum^N_{i=1}(y_i-\hat y_i)^2}{N}} $$
    - **Subtask 2**
    
    $$ Accuracy=\frac{C}{N}, Reward=\frac{1}{N}\sum^N_{i=1}(\cal{1}_{\hat y_i=y_i}-\mathbb{1}_{\hat y_i\neq y_i})|f_i^{(1)}-f_i^{(2)}|  $$

- **Benchmarks**
    - **BASELINE:** assigns the mean rating (Subtask 1) or the majority label (Subtask 2) from the training set.
    - **CBOW:** the context independent word representations obtained using the pretrained GloVe word vectors with 300d embeddings and a dictionary of 2.2M words.
    - **BERT:** a regressor based on BERT base model embeddings (Devlin et al., 2019).
    - **RoBERTa:** same regressor as above but uses RoBERTa embeddings (Liu et al., 2019).
    For a thorough discussion of these benchmarks, we refer the reader to the Duluth system (Jin et al., 2020), who performed these ablation experiments. In summary, each benchmark result uses the edited headline, **CONTEXT** implies using the headline’s context (with the replaced word substituted with [MASK]), **ORIG** implies using the original headline, **FT** refers to finetuning, **FREEZE** implies feature extraction (no finetuning) and **FUNLINES** refers to using the FunLines training data.
    <div align="center">
        <image src="img/2.png" width="700">
    </div>

- **Results**
    - **Subtask 1**
    <div align="center">
        <image src="img/3.png" width="700">
    </div>

    - **Subtask 2**
    <div align="center">
        <image src="img/4.png" width="700">
    </div>

### Overview of Participating Systems
- **Reuse of SubTask 1 System for Subtask 2**
First, we note that for Subtask 2, most systems relied on the model they developed for Subtask 1. This involved using the model to estimate a real number funniness rating for each of the two edited headlines, and selecting the one which achieved the higher estimated rating. As a result, there was a strong correlation between teams’ placements in Subtask 1 and Subtask 2, with the top 3 teams in both tasks being the same.

- **The Hitachi System**
The winner of both tasks, Hitachi (Morishita et al., 2020), formulated the problem as sentence pair regression and exploited an ensemble of the PLMs BERT, GPT-2, RoBERTa, XLNet, Transformer-XL and XLM. Their training data uses the pairs of headlines, with the replacement word marked with special tokens, and they fine-tune 50 instances per PLM, each having a unique hyperparameter setting. After applying 5-fold cross validation, they selected the 20 best performing settings per PLM, for a total of 700 PLMs (7 PLMs * 20 hyperparameters * 5 folds). They combined the predictions of these models via Ridge regression in the ensemble to predict final funniness scores. Hitachi uses the additional training data from FunLines.

- **The Amobee System**
Amobee (Rozental et al., 2020) was the 2nd placed team for both Subtasks. Using PLM token embeddings, they trained 30 instances of BERT, RoBERTa and XLNet, combining them for an ensemble of 90 models.

- **The YNU-HPCC System**
Unlike the top two systems, the 3rd placed YNU-HPCC (Tomasulo et al., 2020) employed an ensemble method that uses only the edited headlines. They used multiple pre-processing methods (e.g., cased vs uncased, with or without punctuation), and they encoded the edited headlines using FastText, Word2Vec, ELMo and BERT encoders. The final ensemble consists of 11 different encodings (four FastText, two W2V, four Bert, one ELMo). For each of these encodings, a bidirectional GRU was trained using the encoded vectors. In the ensemble, the GRU predictions were concatenated and fed to an XGBoost regressor.

- **MLEngineer**
The MLEngineer (Shatnawi et al., 2020) team also used only the edited headlines. They fine-tune and combine four BERT sentence regression models to estimate a rating, and they combine it with the estimated rating from a model that incorporates RoBERTa embeddings and a Na¨ıve Bayes regressor to generate the final rating.

- **The LMML and ECNU Systems**
These systems (Ballapuram, 2020; Zhang et al., 2020) estimate the funniness of headlines using a neural architecture that focuses on the importance of the replaced and replacement words against the contextual words in the headline. They use BERT embeddings and compute feature vectors based on the global attention between the contextual words and the replaced (and replacement) word. These two vectors and the vectors of the replaced and replacement are combined, and the resulting vector is passed through a multi-layer perceptron to estimate the headline’s funniness.

- **Other Notable Approaches**
***ECNU*** used sentiment and humor lexicons, respectively, to extract polarities and humor rating features of headlines. They also used the average, minimum and maximum humor ratings of replaced/replacement words from the training set as additional features.
***LT3*** (Vanroy et al., 2020) created an entirely featureengineered baseline which obtained an RMSE of 0.572. It uses lexical, entity, readability, length, positional, word embedding similarity, perplexity and string similarity features.
***IRLab DAIICT*** trained five BERT classifiers, one for each of the five ratings for a headline, and calculated the mean of the five classifiers’ outputs. This mean was further averaged with the output of a BERT regression model which predicts the overall mean rating.
***Buhscitu*** (Jensen et al., 2020) used knowledge bases (e.g. WordNet), a language model and hand-crafted features (e.g. phoneme level distances). Their neural model combines feature, knowledge and word (replaced/ replacement) encoders.
***Hasyarasa*** (Desetty et al., 2020) used a word embedding and knowledge graph based approach to build a contextual neighborhood of words to exploit entity interrelationships and to capture contextual absurdity. Features from this and semantic distance based features are finally combined with headline representations from a Bi-LSTM.
***UTFPR*** (Paetzold, 2020) is a minimalist unsupervised approach that uses word co-occurrence features derived from news and EU parliament transcripts to capture unexpectedness.

### Analysis and Discussion
- **Subtask 1 (Regression)**
To better understand which funniness ranges are particularly hard for systems to assess, we study the performance of the systems as a function of ground truth funniness. As shown in Figure 2, we grouped the edited headlines into funniness bins of width 0.2. For each bin, we plotted the mean absolute regression errors for the top 20 systems aggregated (max RMSE = 0.547), the winning Hitachi system (RMSE = 0.497), the 19 other systems and BASELINE (RMSE = 0.575).
    <div align="center">
        <image src="img/5.png" width="700">
    </div>

    Figure 3 shows the systems’ antipodal RMSE, an auxiliary metric for Subtask 1, which we calculated by considering only the $X\%$ most funny headlines and $X\%$ least funny headlines, for $X \in \{10, 20, 30, 40\}$ in the RMSE metric. The systems are ranked by their overall RMSE for Subtask 1. It appears that some of the systems further down the ranking are doing much better at estimating the funniness of the extremes in the dataset than their superiors. For example, the large dip shows the system ranked 41 (Hahackathon) is performing better at estimating the funniness of the top 10-40% most/least funny headlines than several systems ranked before it. This suggests that combining these approaches can yield better results, for example, using some selected systems to rank certain subsets of headlines.
    <div align="center">
        <image src="img/6.png" width="700">
    </div>

- **Subtask 2 (Classification)**
    - **Incongruity at Play**
    The dashed curve in Figure 4(a) shows the incongruity measure obtained using GloVe word distances:
    $$ \tt{incongruity\ difference} = distance(orig, edit_2) - distance(orig, edit_1) $$
    $$ \tt incongruity\ measure = correlation(incongruity\ difference, ground\ truth\ label \in \{1, 2\})$$
    <div align="center">
        <image src="img/7.png" width="700">
    </div>   

    - **Funniness Gaps**
    <div align="center">
        <image src="img/8.png" width="700">
    </div>   

    - **Extreme Examples**
    <div align="center">
        <image src="img/9.png" width="700">
    </div>  
