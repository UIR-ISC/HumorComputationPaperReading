# Duluth at SemEval-2017 Task 6: Language Models in Humor Detection

在给定的两条Twitter消息中确定哪一条更有趣、对推特的幽默程度打分。使用KenLM语言模型（bigram、trigram）。

#### Author

- **Xinru Yan, Ted Pedersen**: Department of Computer Science, University of Minnesota Duluth

#### Abstract

This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.

> ### Overview
>
> **Model Name**:
>
> - N-gram
>
> **Motivation**:
>
> - 在SemEval-2017 Task 6 “#HashtagWars: Learning a Sense of Humor”中的子任务A(比较两条文本幽默度)，取得了第三
>
> **Structure**:
>
> - bigram/trigram
>
> **Method**:
>
> - Corpus preparation and pre-processing
> - Language model training
> - Tweet scoring
> - Tweet prediction
>
> **Experiment**:
>
> - 在最终结果中取得了第三
>
> **Brief Comment**:
>
> - 对比不同的N-gram以及预处理对结果的影响

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.
- **Subtask A:** Given two tweets, compare and predict which one is funnier.
- **Subtask B:** Given a set of tweets associated with one hashtag, rank tweets from the funniest to the least funny.

### Background
In order to evaluate how funny a tweet is, we train language models on two datasets: the tweet data and the news data. Tweets that are more probable according to the tweet data language model are ranked as being funnier. However, tweets that have a lower probability according to the news language model are considered the funnier since they are the least like the (unfunny) news corpus. We relied on both bigrams and trigrams when training our models.
We use KenLM (Heafield et al., 2013) as our language modeling tool. Language models are estimated using modified Kneser-Ney smoothing without pruning. KenLM also implements a backoff technique so if an N-gram is not found, KenLM applies the lower order N-gram’s probability along with its back-off weights.

### Method
- **Corpus Preparation and Pre-processing**
    - **Preparation**
    During the development of our system we trained our language models solely on the 100 hashtag files from train dir and then evaluated our performance on the 6 hashtag files found in trial dir. That data was formatted such that each tweet was found on a single line.
    - **Pre-processing**
        - Filtering removes the following elements from the tweets: URLs, tokens starting with the “@” symbol (Twitter user names), and tokens starting with the “#” symbol (Hashtags).
        - Tokenization: Text in all training data was split on white space and punctuation
- **Language Model Training**
Once we had the corpora ready, we used the KenLMToolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.
- **Tweet Scoring**
After training the N-gram language models, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. Table 1 shows an example of two scored tweets from hashtag file Bad Job In 5 Words.tsv based on the tweet data trigram language model. Note that KenLMreports the log of the probability of the N-grams rather than the actual probabilities so the value closer to 0 (-19) has the higher probability and is associated with the tweet judged to be funnier.
<div align="center">
    <image src="img/1.png" width="700">
</div>

- **Tweet Prediction**
The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first. If the scores are based on the tweet language model then they are sorted in ascending order since the log probability value closest to 0 indicates the tweet that is most like the (funny) tweets model. However, if the log probability scores are based on the news data then they are sorted in descending order since the largest value will have the smallest probability associated with it and is therefore least like the (unfunny) news model.
\
For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet ids for the pair followed by a “1”. If the second tweet is funnier it outputs the tweet ids followed by a “0”. For Subtask B, the system outputs all the tweet ids for a hashtag file starting from the funniest.

### Experiment
Table 2 shows results from the development stage. These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization. While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548). We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543). For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive. Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.
<div align="center">
    <image src="img/2.png" width="700">
</div>

Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post–evaluation runs than the tweet data.
<div align="center">
    <image src="img/3.png" width="700">
</div>
