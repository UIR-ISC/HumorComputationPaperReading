# LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits based Humor Grading

评估了BERT及其衍生模型（RoBERTa、DistilBERT和ALBERT）对基于简短编辑的幽默评分的能力。在Humicroedit和FunLines数据集上测试这些幽默分级和分类模型。

#### Author

- **Siddhant Mahurkar:** Department of Computer Science & Engineering, Vellore Institute of Technology
- **Rajaswa Patil:** Department of Electrical & Electronics Engineering, BITS Pilani K. K. Birla Goa Campus

#### Abstract

In this paper, we assess the ability of BERT and its derivative models (RoBERTa, DistilBERT, and ALBERT) for short-edits based humor grading. We test these models for humor grading and classification tasks on the Humicroedit and the FunLines dataset. We perform extensive experiments with these models to test their language modeling and generalization abilities via zero-shot inference and cross-dataset inference based approaches. Further, we also inspect the role of self-attention layers in humor-grading by performing a qualitative analysis over the self-attention weights from the final layer of the trained BERT model. Our experiments show that all the pre-trained BERT derivative models show significant generalization capabilities for humor-grading related tasks.

> ### Overview
>
> **Model Name**:
>
> - BERT, RoBERTa, DistilBERT, ALBERT
>
> **Motivation**:
>
> - 对比不同的模型对幽默标题评分的影响
>
> **Structure**:
>
> - RoBERTa
>
> **Method**:
>
> - Masked-LM Finetuning
>
> **Experiment**:
>
> - 对比不同的BERT以及Finetuning的影响
>
> **Brief Comment**:
>
> - 再不同的数据集上，对比了不同的BERT的效果

<!-- ATTENTION! BE SURE TO ENSURE THE AVAILABILITY OF YOUR NOTES -->

### Introduction
In this work, we propose using BERT (Devlin et al., 2019) and its derivative models (collectively referred to as ”BERT models” further throughout the paper) for short-edits based humor-grading. We mainly follow two approaches: In the first one we use pre-trained BERT models with a final regression layer to get a humor-grade for each edited news-headline, and for the second one we use masked-word language-modeling to fine-tune the pre-trained BERT models on the task dataset, which are further trained for the humor-grade regression task. We compare the results of these two approaches for each sub-task. We also test the generalization capabilities of the BERT models for this task by using the FunLines dataset (Hossain et al., 2020b), which is provided as an additional dataset for this shared-task. The code for this work is made publicly available as a GitHub repository.

### Background
In this work, we test the ability of the BERT models to perform humor-grading tasks on the Humicroedit dataset (official task dataset) and FunLines dataset (additional dataset). Both of these are English shortedits based humor-grading datasets. The Humicroedit dataset consists of 15,095 edited headlines, and the FunLines dataset consists of 8,248 edited headlines. Both the datasets share the same format. Table 1 shows two different edits of the same news-headline with varying grades of humor.
<div align="center">
    <image src="img/1.png" width="700">
</div>  

### Methodology
For sub-task 1, humor grading is considered a regression task over the humor grades of the edited news-headlines. We experiment with two types of weight-initializations for this task: weights from the language model fine-tuned on the dataset and the original pre-trained BERT model weights. We fine-tune the BERT models for the regression task by appending a fully-connected layer. This layer takes in the pooled output embedding from the BERT models and returns a humor grade value. The models are trained with the mean-squared error between the predicted and ground-truth humor grades as an objective function. The model weights are optimized for this objective function by Adam optimizer. For sub-task 2, we directly use the sub-task 1 models for a zero-shot inference over the pairs of edited news-headlines, as shown in Figure 1. After getting the humor grades for both the edits of a news-headline, we label the one with a higher grade value as the more humorous one.
<div align="center">
    <image src="img/2.png" width="700">
</div>  

### Experiment
- **Data**
<div align="center">
    <image src="img/3.png" width="700">
</div>  

- **Results**
<div align="center">
    <image src="img/4.png" width="700">
</div>

- **Attention Visualisation**
Figure 2-1 and Figure 2-3 represent the attention visualization of some of the best predictions from the Humicroedit and FunLines dataset, respectively. In these cases, we observe that the [CLS] token firmly attends the edited words by the 2nd and 12th attention head. On the other hand, Figure 2-2 and Figure 2-4 represent some of the worst predictions from the Humicroedit and FunLines dataset, respectively. Here, the [CLS] token divides its attention relatively more uniformly across the entire input sequence, with relatively lesser attention from the 2nd and 12th attention head. The same trend is observed across most of the samples from both of the datasets for all the BERT models. This reveals the importance of self-attention in the decision making of the BERT models for short-edits based humor grading.

<div align="center">
    <image src="img/5.png" width="700">
</div>
